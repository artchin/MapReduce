Hadoop MapReduce — распределённая обработка Две задачи на обработку больших данных с использованием парадигмы MapReduce:

Задача 1 (Random ID Shuffle): Перемешивание списка идентификаторов в случайном порядке с последующей группировкой от 1 до 5 ID в строке. Алгоритм: присвоение случайного ключа каждому ID → распределённая сортировка → группировка со случайным размером batch.

Задача 2 (Bigram Document Frequency): Подсчёт пар слов (биграмм), встречающихся в наибольшем числе документов Wikipedia (4000+ статей). Очистка от пунктуации (regex [^A-Za-z\\s]), приведение к нижнему регистру, подсчёт уникальных документов для каждой биграммы, сортировка по частоте и лексикографически.

Stack: Hadoop (HDFS, YARN, MapReduce), Python Streaming API, регулярные выражения
